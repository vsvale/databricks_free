{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b8344d7-a853-4952-b45b-0a75f4626ca1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.compute import InstancePoolAwsAttributes,InstancePoolAccessControlRequest,InstancePoolPermissionLevel\n",
    "from pyspark.sql.functions import sum as _sum, when,col, split\n",
    "import pandas as pd\n",
    "\n",
    "w = WorkspaceClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29d024c3-080f-4efa-a2f6-d028f24f226c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (spark.sql(\"\"\"\n",
    "  with node_counts as\n",
    "(SELECT\n",
    "    driver_node_type as node_type, 1 as node_count\n",
    "  FROM\n",
    "    system.compute.clusters\n",
    "WHERE \n",
    "workspace_id = dataops_prd.libs.get_workspace_id() AND driver_instance_pool_id is null\n",
    "UNION ALL\n",
    "SELECT\n",
    "    worker_node_type as node_type, coalesce(worker_count,max_autoscale_workers) as node_count\n",
    "  FROM\n",
    "    system.compute.clusters\n",
    "WHERE workspace_id = dataops_prd.libs.get_workspace_id() and worker_instance_pool_id is null\n",
    ")\n",
    "select node_type, sum(node_count) as count\n",
    "FROM\n",
    "    node_counts\n",
    "GROUP BY ALL\n",
    "ORDER BY count DESC\n",
    "\"\"\"))\n",
    "df = df.select(\n",
    "    split(col(\"node_type\"), \"\\.\").getItem(0).alias('instance_family'),\n",
    "    split(col(\"node_type\"), \"\\.\").getItem(1).alias('size'),\n",
    "    col(\"count\")\n",
    "  )\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "213a5f5f-bf91-473d-9643-948db54bd67c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, concat, lit\n",
    "\n",
    "fleet_instances = df.select(\n",
    "    concat(regexp_replace(\"instance_family\", r\"\\d+\", \"\"), lit(\"-fleet\")).alias(\"instance_family\")\n",
    ").withColumn('size', lit(None)).withColumn('count', lit(None))\n",
    "\n",
    "display(fleet_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f14e3cd7-72a6-4cf8-b2b3-ac0bcf01f99a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "instances_to_pools = df.unionByName(fleet_instances)\n",
    "instances_to_pools.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84b813ca-df18-47b1-a609-5db54f163b3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "most_used_instance = df.groupBy(\"instance_family\").agg(_sum(\"count\"))\n",
    "\n",
    "instances_to_change = (\n",
    "    most_used_instance\n",
    "    .join(instances_to_pools, instances_to_pools[\"instance_family\"] == most_used_instance[\"instance_family\"], \"left\")\n",
    "    .withColumn(\"saving_plan\", when(instances_to_pools[\"instance_family\"].isNull(), False).otherwise(True))\n",
    "    .where(\"saving_plan = false\")\n",
    "    .select(most_used_instance[\"instance_family\"],\"sum(count)\",\"saving_plan\")\n",
    ")\n",
    "\n",
    "display(instances_to_change)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e49a2ff-8ac5-4f15-b440-99a5fb848df2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "most_used_size_list = df.select(\"size\").distinct().selectExpr(\"collect_list(size)\").first()[0]\n",
    "instances_to_pools_list = instances_to_pools.selectExpr(\"collect_list(instance_family)\").first()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09f8b837-1f7b-41ae-95fd-0cb9541c485b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "node_type_id_list = [\n",
    "    f\"{instance}.{size}\"\n",
    "    for instance in instances_to_pools_list\n",
    "    for size in most_used_size_list\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bada45f8-ff26-48aa-9ad5-35a5330c090c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "node_types = w.clusters.list_node_types().node_types\n",
    "df = pd.DataFrame(node_types)\n",
    "node_df = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0e2a9d9-c302-4dcc-8293-ef7960ece922",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "zones = w.clusters.list_zones().zones\n",
    "\n",
    "for node_type_id in node_type_id_list:\n",
    "  try:\n",
    "    if node_df.where(f\"node_type_id = '{node_type_id}'\").count() > 0:\n",
    "      category = node_df.where(f\"node_type_id = '{node_type_id}'\").selectExpr(\"replace(lower(category), ' ', '_')\").first()[0]\n",
    "      for avaiability in [\"SPOT\",\"ON_DEMAND\"]:\n",
    "        if \"fleet\" in node_type_id:\n",
    "          zones = [\"auto\"]\n",
    "        for zone_id in zones:\n",
    "          aws_attributes = InstancePoolAwsAttributes.from_dict(\n",
    "              {\n",
    "                \"availability\": avaiability,\n",
    "                \"zone_id\": zone_id,\n",
    "                \"spot_bid_price_percent\": 100 if avaiability == \"SPOT\" else None\n",
    "              }\n",
    "          )\n",
    "          w.instance_pools.create(\n",
    "            node_type_id=node_type_id,\n",
    "            instance_pool_name=f\"{avaiability.lower()}_{zone_id.replace('-', '_')}_{category}_{node_type_id}_pool\",\n",
    "            idle_instance_autotermination_minutes=10,\n",
    "            # max_capacity=100000,\n",
    "            min_idle_instances=0,\n",
    "            enable_elastic_disk=True,\n",
    "            aws_attributes=aws_attributes\n",
    "          )\n",
    "    else:\n",
    "      print(f\"Node type {node_type_id} does not exist\")\n",
    "  except Exception as e:\n",
    "    print(f\"Error creating instance pool for {node_type_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8254b57c-f35a-4977-a55a-2e11b421ea8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_pools_pd = pd.DataFrame([*map(lambda x: x.as_dict(), w.instance_pools.list())])\n",
    "all_pools_df = spark.createDataFrame(all_pools_pd)\n",
    "\n",
    "display(all_pools_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03c6ed89-ee8d-4575-9ac5-9d9850dc795f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pools_to_permit = all_pools_df.selectExpr(\"collect_list(instance_pool_id)\").first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "961c4ce0-429b-4c68-ab96-e8644b59b114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for instance_pool_id in pools_to_permit:\n",
    "  try:\n",
    "    w.instance_pools.update_permissions(instance_pool_id=instance_pool_id, access_control_list = [\n",
    "      InstancePoolAccessControlRequest(group_name=\"admins\",\n",
    "    permission_level = InstancePoolPermissionLevel(\"CAN_MANAGE\")\n",
    "),\n",
    "      InstancePoolAccessControlRequest(group_name=\"users\",\n",
    "    permission_level = InstancePoolPermissionLevel(\"CAN_ATTACH_TO\")\n",
    ")\n",
    "      ]\n",
    "                                        )\n",
    "  except Exception as e:\n",
    "    print(f\"Error updating permissions for {instance_pool_id}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pool",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

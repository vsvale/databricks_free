{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7fe4bbb-91e6-40da-b032-c58654573014",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "##\n",
    "- Group of pre-configured, idle virtual machines that are ready to be assigned to clusters as needed\n",
    "- Optimize resource usage\n",
    "- Respect cloud provider VCPU quotas\n",
    "- Reduce operational latency, **min_idle_instances** and **idle_instance_autotermination_minutes** can minimize the time it takes to spin up clusters\n",
    "- Reduce in both cluster start and autoscaling time\n",
    "- Cloud provider charge for the idle instances in a pool, Databricks does not charge\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1582f14-52c4-4cc7-a7b6-1781c5faf8bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- https://docs.databricks.com/aws/en/compute/pool-index\n",
    "- [Pool configuration reference](https://docs.databricks.com/aws/en/compute/pools)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5173c72a-5b04-4f02-8532-56cd09630b30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- [databricks sdk instance pools](https://databricks-sdk-py.readthedocs.io/en/latest/workspace/compute/instance_pools.html)\n",
    "- [Terraform data instance pools](https://registry.terraform.io/providers/databricks/databricks/latest/docs/data-sources/instance_pool)\n",
    "- [Terraform resource instance pools](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/instance_pool)\n",
    "- [Dabricks CLI instance pools](https://docs.databricks.com/aws/en/dev-tools/cli/reference/instance-pools-commands)\n",
    "- [Databricks API instance pools](https://docs.databricks.com/api/workspace/instancepools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "328fd41c-0499-42b6-86e4-5e4f06e44624",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> ℹ️ If your workload supports serverless compute, Databricks recommends using serverless compute instead of pools to take advantage of always-on, scalable compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b8344d7-a853-4952-b45b-0a75f4626ca1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.compute import InstancePoolAwsAttributes,InstancePoolAccessControlRequest,InstancePoolPermissionLevel\n",
    "from pyspark.sql.functions import sum as _sum, when,col, split, regexp_replace, concat, lit\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, StringType,LongType,BooleanType\n",
    "\n",
    "w = WorkspaceClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29d024c3-080f-4efa-a2f6-d028f24f226c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "most used computes not vinculated to pools"
    }
   },
   "outputs": [],
   "source": [
    "df = (spark.sql(\"\"\"\n",
    "  with node_counts as\n",
    "(SELECT\n",
    "    driver_node_type as node_type, 1 as node_count\n",
    "  FROM\n",
    "    system.compute.clusters\n",
    "WHERE \n",
    "workspace_id = dataops_prd.libs.get_workspace_id() AND driver_instance_pool_id is null\n",
    "UNION ALL\n",
    "SELECT\n",
    "    worker_node_type as node_type, coalesce(worker_count,max_autoscale_workers) as node_count\n",
    "  FROM\n",
    "    system.compute.clusters\n",
    "WHERE workspace_id = dataops_prd.libs.get_workspace_id() and worker_instance_pool_id is null\n",
    ")\n",
    "select node_type, sum(node_count) as count\n",
    "FROM\n",
    "    node_counts\n",
    "GROUP BY ALL\n",
    "ORDER BY count DESC\n",
    "\"\"\"))\n",
    "df = df.select(\n",
    "    split(col(\"node_type\"), \"\\.\").getItem(0).alias('instance_family'),\n",
    "    split(col(\"node_type\"), \"\\.\").getItem(1).alias('size'),\n",
    "    col(\"count\")\n",
    "  )\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "213a5f5f-bf91-473d-9643-948db54bd67c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "gerate fleet equivalent"
    }
   },
   "outputs": [],
   "source": [
    "fleet_instances = df.select(\n",
    "    concat(regexp_replace(\"instance_family\", r\"\\d+\", \"\"), lit(\"-fleet\")).alias(\"instance_family\")\n",
    ").withColumn('size', lit(None)).withColumn('count', lit(None))\n",
    "\n",
    "display(fleet_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f14e3cd7-72a6-4cf8-b2b3-ac0bcf01f99a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "instance to create pools"
    }
   },
   "outputs": [],
   "source": [
    "instances_to_pools = df.unionByName(fleet_instances)\n",
    "instances_to_pools.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84b813ca-df18-47b1-a609-5db54f163b3c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "instances to change to pools"
    }
   },
   "outputs": [],
   "source": [
    "most_used_instance = df.groupBy(\"instance_family\").agg(_sum(\"count\"))\n",
    "\n",
    "instances_to_change = (\n",
    "    most_used_instance\n",
    "    .join(instances_to_pools, instances_to_pools[\"instance_family\"] == most_used_instance[\"instance_family\"], \"left\")\n",
    "    .withColumn(\"saving_plan\", when(instances_to_pools[\"instance_family\"].isNull(), False).otherwise(True))\n",
    "    .where(\"saving_plan = false\")\n",
    "    .select(most_used_instance[\"instance_family\"],\"sum(count)\",\"saving_plan\")\n",
    ")\n",
    "\n",
    "display(instances_to_change)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e49a2ff-8ac5-4f15-b440-99a5fb848df2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "most_used_size_list = df.select(\"size\").distinct().selectExpr(\"collect_list(size)\").first()[0]\n",
    "instances_to_pools_list = instances_to_pools.selectExpr(\"collect_list(instance_family)\").first()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09f8b837-1f7b-41ae-95fd-0cb9541c485b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "node_type_id_list = [\n",
    "    f\"{instance}.{size}\"\n",
    "    for instance in instances_to_pools_list\n",
    "    for size in most_used_size_list\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bada45f8-ff26-48aa-9ad5-35a5330c090c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "instance informations"
    }
   },
   "outputs": [],
   "source": [
    "node_types = w.clusters.list_node_types().node_types\n",
    "df = pd.DataFrame(node_types)\n",
    "node_df = spark.createDataFrame(df)\n",
    "\n",
    "node_df.write.mode(\"overwrite\").saveAsTable(\"dataops_prd.workspace.node_types_informations\")\n",
    "display(node_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34441ccf-5dd0-4fac-abff-a2c310e15008",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- **min_idle_instance**: The minimum number of idle instances the pool maintains, which do not terminate regardless of auto termination settings, and are replenished by Databricks if consumed by a cluster.\n",
    "- **max_capacity**: constrains all instances (idle + used). Use to control cost, split instances capacity to stay under cloud provider quota\n",
    "- **idle_instance_autotermination_minutes**: The number of minutes beyond the Minimum Idle Instances setting that idle instances can remain before the pool terminates them.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0e2a9d9-c302-4dcc-8293-ef7960ece922",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "zones = w.clusters.list_zones().zones\n",
    "\n",
    "for node_type_id in node_type_id_list:\n",
    "  try:\n",
    "    if node_df.where(f\"node_type_id = '{node_type_id}'\").count() > 0:\n",
    "      category = node_df.where(f\"node_type_id = '{node_type_id}'\").selectExpr(\"replace(lower(category), ' ', '_')\").first()[0]\n",
    "      for avaiability in [\"SPOT\",\"ON_DEMAND\"]:\n",
    "        if \"fleet\" in node_type_id:\n",
    "          zones = [\"auto\"]\n",
    "        for zone_id in zones:\n",
    "          aws_attributes = InstancePoolAwsAttributes.from_dict(\n",
    "              {\n",
    "                \"availability\": avaiability,\n",
    "                \"zone_id\": zone_id,\n",
    "                \"spot_bid_price_percent\": 100 if avaiability == \"SPOT\" else None\n",
    "              }\n",
    "          )\n",
    "          w.instance_pools.create(\n",
    "            node_type_id=node_type_id,\n",
    "            instance_pool_name=f\"{avaiability.lower()}_{zone_id.replace('-', '_')}_{category}_{node_type_id}_pool\",\n",
    "            idle_instance_autotermination_minutes=10,\n",
    "            # max_capacity=100000,\n",
    "            min_idle_instances=0,\n",
    "            enable_elastic_disk=True,\n",
    "            aws_attributes=aws_attributes\n",
    "          )\n",
    "    else:\n",
    "      print(f\"Node type {node_type_id} does not exist\")\n",
    "  except Exception as e:\n",
    "    print(f\"Error creating instance pool for {node_type_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6d7079d-1267-474f-8c59-5adca5994662",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_pools_df_schema = StructType([\n",
    "    StructField('aws_attributes', StructType([\n",
    "        StructField('availability', StringType(), True),\n",
    "        StructField('spot_bid_price_percent', LongType(), True),\n",
    "        StructField('zone_id', StringType(), True)\n",
    "    ]), True),\n",
    "    StructField('default_tags', StructType([\n",
    "        StructField('DatabricksInstanceGroupId', StringType(), True),\n",
    "        StructField('DatabricksInstancePoolCreatorId', StringType(), True),\n",
    "        StructField('DatabricksInstancePoolId', StringType(), True),\n",
    "        StructField('Vendor', StringType(), True)\n",
    "    ]), True),\n",
    "    StructField('enable_elastic_disk', BooleanType(), True),\n",
    "    StructField('idle_instance_autotermination_minutes', LongType(), True),\n",
    "    StructField('instance_pool_id', StringType(), True),\n",
    "    StructField('instance_pool_name', StringType(), True),\n",
    "    StructField('min_idle_instances', LongType(), True),\n",
    "    StructField('node_type_id', StringType(), True),\n",
    "    StructField('state', StringType(), True),\n",
    "    StructField('stats', StructType([\n",
    "        StructField('idle_count', LongType(), True),\n",
    "        StructField('pending_idle_count', LongType(), True),\n",
    "        StructField('pending_used_count', LongType(), True),\n",
    "        StructField('used_count', LongType(), True)\n",
    "    ]), True),\n",
    "    StructField('status', StructType([\n",
    "        StructField(\"instance_id\", StringType(), True),\n",
    "        StructField(\"message\", StringType(), True)\n",
    "    ]), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8254b57c-f35a-4977-a55a-2e11b421ea8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_pools_pd = pd.DataFrame([*map(lambda x: x.as_dict(), w.instance_pools.list())])\n",
    "all_pools_df = spark.createDataFrame(all_pools_pd, schema=all_pools_df_schema)\n",
    "all_pools_df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\"dataops_prd.workspace.instance_pools\")\n",
    "display(all_pools_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03c6ed89-ee8d-4575-9ac5-9d9850dc795f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pools_to_permit = all_pools_df.selectExpr(\"collect_list(instance_pool_id)\").first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "961c4ce0-429b-4c68-ab96-e8644b59b114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for instance_pool_id in pools_to_permit:\n",
    "  try:\n",
    "    w.instance_pools.update_permissions(instance_pool_id=instance_pool_id, access_control_list = [\n",
    "      InstancePoolAccessControlRequest(group_name=\"admins\",\n",
    "    permission_level = InstancePoolPermissionLevel(\"CAN_MANAGE\")\n",
    "),\n",
    "      InstancePoolAccessControlRequest(group_name=\"users\",\n",
    "    permission_level = InstancePoolPermissionLevel(\"CAN_ATTACH_TO\")\n",
    ")\n",
    "      ]\n",
    "                                        )\n",
    "  except Exception as e:\n",
    "    print(f\"Error updating permissions for {instance_pool_id}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pool",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

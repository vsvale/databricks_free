{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd38c0ce-ff45-4072-8ac4-779a574e2883",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## \n",
    "- **Usage**: Interactive development and ad hoc analysis\n",
    "- **Management**: Manged by the user via UI, CLI or API REST\n",
    "- **Termination**: Manual or Auto-termination after inactivity. 120 min (default), 10 min (minimun)\n",
    "- **Cost efficiency**: cost more to run when compared to job and sql warehouse pro. $0.65 per DBU classic and $0.95 per DBU serveless \n",
    "    https://www.databricks.com/product/pricing/datascience-ml\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52b46ac2-ac92-4cf4-b227-7a0d2c25b81b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "##  Serveless Compute for Notebooks\n",
    "- On demand\n",
    "- Auto scalable\n",
    "- Run SQL and Python\n",
    "- No infra\n",
    "- Budget Policies\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "452885b3-cc26-40de-b3bc-5c3fb5b62a35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "##  Single-node clustrre\n",
    "- Just driver node\n",
    "- Driver handles both driver and worker responsabilities\n",
    "- Cost effective\n",
    "- Run SQL and Python\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ef5c1dd-42b7-4579-9808-e6069445d9ac",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "create all purpose cluster"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "print(\"Attempting to create cluster. Please wait...\")\n",
    "\n",
    "c = w.clusters.create_and_wait(\n",
    "  cluster_name             = 'my-cluster',\n",
    "  spark_version            = '13.3.x-scala2.12',\n",
    "  node_type_id             = 'i3.xlarge',\n",
    "  autotermination_minutes  = 15,\n",
    "  num_workers              = 1\n",
    ")\n",
    "\n",
    "print(f\"The cluster is now ready at \" \\\n",
    "      f\"{w.config.host}#setting/clusters/{c.cluster_id}/configuration\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cd51f71-3b4b-4549-b146-423d8913bc30",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "list all all purpose cluster"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "w = WorkspaceClient()\n",
    " \n",
    "for c in w.clusters.list():\n",
    "  print(c.cluster_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "037b53b0-0a90-4032-9249-07ab7dfd2091",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Identify cost of all purpose clusters this month vs last month"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fce54bea-0288-490e-bd84-8302fd9f87f5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "most used node types in workspace"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "with node_counts as\n",
    "(SELECT\n",
    "    driver_node_type as node_type, 1 as node_count\n",
    "  FROM\n",
    "    system.compute.clusters\n",
    "WHERE \n",
    "workspace_id = dataops_prd.libs.get_workspace_id()\n",
    "UNION ALL\n",
    "SELECT\n",
    "    worker_node_type as node_type, coalesce(worker_count,max_autoscale_workers) as node_count\n",
    "  FROM\n",
    "    system.compute.clusters\n",
    "WHERE workspace_id = dataops_prd.libs.get_workspace_id()\n",
    ")\n",
    "select node_type, sum(node_count) as count\n",
    "FROM\n",
    "    node_counts\n",
    "GROUP BY ALL\n",
    "ORDER BY count DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a64e20a5-d3d6-4c1c-a7cd-70e3d1eff7ee",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "most used node type not associated with pools"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "with node_counts as\n",
    "(SELECT\n",
    "    driver_node_type as node_type, 1 as node_count\n",
    "  FROM\n",
    "    system.compute.clusters\n",
    "WHERE \n",
    "workspace_id = dataops_prd.libs.get_workspace_id() AND driver_instance_pool_id is null\n",
    "UNION ALL\n",
    "SELECT\n",
    "    worker_node_type as node_type, coalesce(worker_count,max_autoscale_workers) as node_count\n",
    "  FROM\n",
    "    system.compute.clusters \n",
    "WHERE workspace_id = dataops_prd.libs.get_workspace_id() and worker_instance_pool_id is null\n",
    ")\n",
    "select node_type, sum(node_count) as count\n",
    "FROM\n",
    "    node_counts\n",
    "GROUP BY ALL\n",
    "ORDER BY count DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a6c63c5-901d-4256-b787-0c8577340f00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pyspark.sql.functions import col\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service import workspace\n",
    "from databricks.sdk.service import jobs\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType, LongType, FloatType, BooleanType, TimestampType\n",
    "\n",
    "days_back = 1\n",
    "w = WorkspaceClient()\n",
    "\n",
    "from dataclasses import is_dataclass, asdict\n",
    "\n",
    "def safe_getattr(obj, attr, default=None):\n",
    "    try:\n",
    "        # Convert dataclass to dict if necessary\n",
    "        if is_dataclass(obj):\n",
    "            obj = asdict(obj)\n",
    "\n",
    "        for a in attr.split('.'):\n",
    "            if isinstance(obj, dict):\n",
    "                obj = obj.get(a)\n",
    "            else:\n",
    "                obj = getattr(obj, a, None)\n",
    "\n",
    "        # Attempt to cast obj to the type of default, if default is not None\n",
    "        if default is not None and obj is not None:\n",
    "            obj_type = type(default)\n",
    "            try:\n",
    "                return obj_type(obj)\n",
    "            except (ValueError, TypeError):\n",
    "                return default\n",
    "        return obj if obj is not None else default\n",
    "    except (AttributeError, KeyError):\n",
    "        return default\n",
    "\n",
    "def process_job_object(job_obj):\n",
    "    row = {\n",
    "        \"job_id\": safe_getattr(job_obj, \"job_id\"),\n",
    "        \"created_time\": safe_getattr(job_obj, \"created_time\"),\n",
    "        \"creator_user_name\": safe_getattr(job_obj, \"creator_user_name\", \"None\"),\n",
    "        \"name\": safe_getattr(job_obj, \"settings.name\"),\n",
    "        \"parameters\": safe_getattr(job_obj, \"settings.parameters\"),\n",
    "        \"schedule_paused_status\": safe_getattr(job_obj, \"settings.schedule.pause_status.value\", \"None\"),\n",
    "        \"schedule_quartz_cron_expression\": safe_getattr(job_obj, \"settings.schedule.quartz_cron_expression\", \"None\"),\n",
    "        \"schedule_timezone_id\": safe_getattr(job_obj, \"settings.schedule.timezone_id\", \"None\"),\n",
    "        \"schedule_paused_status\": safe_getattr(job_obj, \"settings.schedule.pause_status.value\", \"None\"),\n",
    "        \"schedule_continuous\": safe_getattr(job_obj, \"settings.continuous.pause_status.value\", \"None\"),\n",
    "        # Uncomment and adjust the following line if needed\n",
    "        # \"webhook_notifications.on_success\": safe_getattr(jobObj, \"settings.webhook_notifications.on_success\", \"None\"),\n",
    "        \"task_count\": 0 if job_obj.settings.tasks is None else len(job_obj.settings.tasks),\n",
    "        \"job_clusters\": 0 if job_obj.settings.job_clusters is None else len(job_obj.settings.job_clusters),\n",
    "    }\n",
    "    return row\n",
    "\n",
    "def parallel_process_jobs(jobs_gen, max_workers=20):\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        results = executor.map(process_job_object, jobs_gen)\n",
    "        return list(results)\n",
    "\n",
    "# Usage\n",
    "jobs_gen = w.jobs.list(expand_tasks=True)\n",
    "parsed_job_info = parallel_process_jobs(jobs_gen)\n",
    "\n",
    "# Define the schema based on the provided types_dict\n",
    "schema = StructType([\n",
    "    StructField(\"job_id\", LongType()),\n",
    "    StructField(\"created_time\", LongType()),\n",
    "    StructField(\"creator_user_name\", StringType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"parameters\", ArrayType(StringType())), \n",
    "    StructField(\"schedule_paused_status\", StringType()),\n",
    "    StructField(\"schedule_quartz_cron_expression\", StringType()),\n",
    "    StructField(\"schedule_continuous\", StringType()),\n",
    "    StructField(\"schedule_timezone_id\", StringType()),\n",
    "    StructField(\"task_count\", IntegerType()),\n",
    "    StructField(\"job_clusters\", IntegerType())\n",
    "])\n",
    "\n",
    "sparkJobsDF = spark.createDataFrame(data=parsed_job_info, schema=schema)\n",
    "sparkJobsDF.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\"dataops_prd.control.job_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ddbbe13-bca5-432b-8879-09d49da0b6f4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "jobs running in all purpose"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "with clusters as (\n",
    "  select distinct cluster_id, cluster_name\n",
    "  from (\n",
    "    SELECT\n",
    "    *,\n",
    "    ROW_NUMBER() OVER(PARTITION BY workspace_id, cluster_id ORDER BY change_time DESC) as rn\n",
    "  FROM system.compute.clusters where workspace_id = dataops_prd.libraries.get_workspace_id() and (cluster_name not like \"job-%\" and cluster_name not like \"dlt-%\") and delete_time is null\n",
    "  QUALIFY rn=1\n",
    "  )\n",
    "),\n",
    "jobs_info as (\n",
    "  select distinct j.job_id, j.name as job_name, coalesce(ui.userName,ji.creator_user_name) as creator, from_unixtime(ji.created_time / 1000) as creation_date, change_time,\n",
    "  ROW_NUMBER() OVER(PARTITION BY j.job_id ORDER BY change_time DESC) as rn\n",
    "from \n",
    "system.lakeflow.jobs j\n",
    " left join dataops_prd.control.users_id ui on j.creator_id = ui.id\n",
    " inner join dataops_prd.control.job_info ji on j.job_id = ji.job_id\n",
    " where workspace_id = dataops_prd.libraries.get_workspace_id() and j.delete_time is null\n",
    " QUALIFY rn=1\n",
    "),\n",
    "job_tasks_exploded AS (\n",
    "  SELECT\n",
    "    workspace_id,\n",
    "    job_id,\n",
    "    EXPLODE(compute_ids) as cluster_id,\n",
    "    period_start_time,\n",
    "    ROW_NUMBER() OVER(PARTITION BY job_id ORDER BY period_start_time DESC) as rn\n",
    "  FROM system.lakeflow.job_task_run_timeline\n",
    "  where workspace_id = dataops_prd.libraries.get_workspace_id()\n",
    "  QUALIFY rn=1\n",
    ")\n",
    "select distinct j.job_id, j.job_name, j.creator, j.creation_date, t.cluster_id, c.cluster_name\n",
    "from jobs_info as j\n",
    "left join job_tasks_exploded as t on j.job_id = t.job_id and t.period_start_time >= j.change_time\n",
    "inner join clusters as c on t.cluster_id = c.cluster_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed410301-a4ca-4784-9ca1-3cb5170d0aca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Managing cluster logs\n",
    "- **Event log**: records all significant actions related to the cluster, such as when the cluster was created, terminated, edited, scalated or encontered any erros\n",
    "- **Spark UI**: interface for monitoring and debugging job execution, stages and tasks\n",
    "- **Driver logs**: outputs from the notebooks and libraries"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8769107896723891,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "All Purpose",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

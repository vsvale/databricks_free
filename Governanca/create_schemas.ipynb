{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62bd70ce-f1b7-4d96-9dd8-bb4bdc0c1b2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalogs_df = spark.sql(\"show catalogs\").filter(\"catalog not in ('hive_metastore', 'legacy_hive_catalog', 'sample', 'system','__databricks_internal','clearsale','delta_share_test','delta_share_teste','gcp_bq_lakeflow_prd','glue-odin-contatos-catalog','glue-odin-contatos_catalog','clearsale_enriquecimento_contatos','nike_glue_prd','samples')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e624ec35-c79f-40e5-be32-af2647fe18b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "from pyspark.sql.functions import col\n",
    "catalogs_df = catalogs_df.filter(col(\"catalog\").rlike(\"^[o-z]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1aed369-fd12-4d60-b350-1abdc932649b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "catalogs_df = spark.createDataFrame([(\"cross_prd\",)], [\"catalog\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ce17dd2-6638-4d19-8c4a-4cfdaa079772",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_comment(catalog,schema,schema_path):\n",
    "    product = catalog.replace('_prd', '')\n",
    "    match schema:\n",
    "        case \"bronze\":\n",
    "          return f\"To store raw data ingested from {product}. Contains unprocessed data in delta format, PII is already encrypted, serving as the first layer in the data pipeline. Stored in {schema_path}\"\n",
    "        case \"silver\":\n",
    "          return f\"To store cleansed and enriched data from {product}. Contains data that has been refined and partially processed, making it suitable for more detailed analysis. Stored in {schema_path}\"\n",
    "        case \"gold\":\n",
    "          return f\"To store highly processed and aggregated data from {product}. Contains final datasets that are ready for business intelligence and reporting purposes. Stored in {schema_path}\"\n",
    "        case \"control\":\n",
    "          return f\"To store control tables used during data processing. Contains metadata and control tables that guide and manage the processing workflows. Stored in {schema_path}\"\n",
    "        case \"temp\":\n",
    "          return f\"To store temporary tables and volumes from {product}. Contains intermediate datasets that are used for processing and analysis. Stored in {schema_path}\"\n",
    "        case \"raw\":\n",
    "          return f\"Schema for raw data tables and volumes from {product} data product, stored in {schema_path}\"\n",
    "        case \"sensitive\":\n",
    "          return f\"Schema for tables and volumes with unencrypted PII data from {product} data product, stored in {schema_path}\"\n",
    "        case \"sandbox\":\n",
    "          return f\"Schema for tables and volumes used in {product} team for experiments, stored in {schema_path}\"\n",
    "        case \"stage\":\n",
    "          return f\"Schema for tables and volumes used in {product} for staging and snapshots, stored in {schema_path}\"\n",
    "        case \"ingestion\":\n",
    "          return f\"Schema for tables and volumes used in ingestion from {product} data product, stored in {schema_path}\"\n",
    "        case \"delivery\":\n",
    "          return f\"Schema for tables and volumes used in delivery from {product} data product, stored in {schema_path}\"\n",
    "        case \"control\":\n",
    "          return f\"Schema for control tables from {product} data product, stored in {schema_path}\"\n",
    "        case \"vector\":\n",
    "          return f\"Schema for tables and volumes used in vector search index from {product} data product, stored in {schema_path}\"\n",
    "        case \"models\":\n",
    "          return f\"Schema for tables and volumesused to serve data science models for the {product} data product, stored in {schema_path}\"\n",
    "        case \"features\":\n",
    "          return f\"Schema for tables and volumes used in data science features from {product} data product, stored in {schema_path}\"\n",
    "        case _:\n",
    "            return f\"Schema for {schema} tables and volumes from {product} data product, stored in {schema_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26948d5d-0bfe-4be1-a665-814349e86277",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_schema(catalog,schema):\n",
    "  schema_exists = spark.sql(f\"SHOW SCHEMAS IN {catalog} LIKE '{schema}'\").count() > 0\n",
    "  if schema_exists:\n",
    "    print(f\"Schema {schema} already exist in {catalog}\")\n",
    "  else:\n",
    "    spark.sql(f\"USE CATALOG {catalog}\")\n",
    "\n",
    "    catalog_path = spark.sql(f\"DESCRIBE CATALOG EXTENDED {catalog}\").filter(\"info_name = 'Storage Root'\").select(\"info_value\").collect()[0][0]\n",
    "    schema_path = f\"{catalog_path}{schema}\"\n",
    "    comment_schema = get_comment(catalog,schema,schema_path)\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema} MANAGED LOCATION '{schema_path}' COMMENT '{comment_schema}'\")\n",
    "    spark.sql(f\"ALTER SCHEMA {catalog}.{schema} OWNER TO `APP-ECSBR-DATABRICKS-ADMIN`\")\n",
    "    spark.sql(f\"ALTER SCHEMA {catalog}.{schema} INHERIT PREDICTIVE OPTIMIZATION\")\n",
    "    print(f\"Schema {schema} created in {catalog}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4859b57d-e8ce-43ac-8d99-60ffcd69a4ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_schemas = ['temp','raw','ingestion','bronze','delivery','control','vector','sensitive','sandbox','stage','silver','gold','models','features','semantic']\n",
    "new_schemas = ['semantic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d199e5d-d4d1-480f-95af-55aca8e4df01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for catalog_row in catalogs_df.select(\"catalog\").collect():\n",
    "  catalog = catalog_row.catalog\n",
    "  print(catalog)\n",
    "  for schema_item in new_schemas:\n",
    "    create_schema(catalog,schema_item)\n",
    "  if schema_item == \"silver\":\n",
    "    spark.sql(f\"GRANT USE SCHEMA, SELECT ON SCHEMA {catalog}.silver TO `account users`\")\n",
    "  if schema_item == \"gold\":\n",
    "    spark.sql(f\"GRANT USE SCHEMA, SELECT ON SCHEMA {catalog}.gold TO `account users`\")\n",
    "  if schema_item == \"models\":\n",
    "    spark.sql(f\"GRANT USE SCHEMA, SELECT ON SCHEMA {catalog}.models TO `account users`\")\n",
    "    spark.sql(f\"GRANT APPLY TAG, EXECUTE, READ VOLUME, REFRESH, SELECT, USE SCHEMA ON SCHEMA {catalog}.models TO `APP-ECSBR_Databricks_DataScience`\")\n",
    "  if schema_item == \"features\":\n",
    "    spark.sql(f\"GRANT APPLY TAG, EXECUTE, READ VOLUME, REFRESH, SELECT, USE SCHEMA ON SCHEMA {catalog}.features TO `APP-ECSBR_Databricks_DataScience`\")\n",
    "  if schema_item == \"semantic\":\n",
    "    spark.sql(f\"GRANT USE SCHEMA, SELECT ON SCHEMA {catalog}.semantic TO `account users`\")\n",
    "  if schema_item == \"raw\":\n",
    "    spark.sql(f\"GRANT APPLY TAG, EXECUTE, MODIFY, READ VOLUME, REFRESH, SELECT, USE SCHEMA ON SCHEMA {catalog}.raw TO `APP-ECSBR_Databricks_Sustentacao`\")\n",
    "  for schema_sp_grant in ['raw','ingestion','bronze','delivery','control','vector','stage','silver','gold','models','features']:\n",
    "    try:\n",
    "      spark.sql(f\"GRANT APPLY TAG, CREATE FUNCTION, CREATE MATERIALIZED VIEW, CREATE MODEL, CREATE MODEL VERSION, CREATE TABLE, CREATE VOLUME, EXECUTE, MODIFY, READ VOLUME,  REFRESH, SELECT, USE SCHEMA, WRITE VOLUME   ON SCHEMA {catalog}.{schema_sp_grant} TO `ecs_ci_cd_datalake@br.experian.com`\")\n",
    "    except Exception as e:\n",
    "      print(f\"Error granting {schema_sp_grant} schema to ecs_ci_cd_datalake@br.experian.com\")\n",
    "\n",
    "  for schema_sp_download in ['temp','raw','ingestion','bronze','delivery','control','vector','sandbox','stage','silver','gold','models','features']:\n",
    "    spark.sql(f\"GRANT USE SCHEMA, SELECT ON SCHEMA {catalog}.silver TO `544cb3ff-43e7-4e1c-bb8c-85c55b27b021`\")\n",
    "  spark.sql(f\"GRANT MANAGE ON SCHEMA {catalog}.sensitive TO `APP-ECSBR_Databricks_Governanca`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22b98a4b-2b43-4978-8b74-2925e4c07d2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for schema_name in ['raw','bronze','silver','gold','sandbox','temp','features','models','semantic']:\n",
    "  if spark.sql(f\"SHOW SCHEMAS IN `{catalog}` LIKE '{schema_name}'\").count() > 0:\n",
    "    spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog}.{schema_name}.checkpoints\")\n",
    "    for principal_name in ['NTT-MIGRATION','ecs_ci_cd_datalake@br.experian.com','APP-ECSBR-DATABRICKS-ADMIN','APP-ECSBR_Databricks_Sustentacao']:\n",
    "      spark.sql(f\"GRANT READ VOLUME, WRITE VOLUME ON VOLUME {catalog}.{schema_name}.checkpoints TO `{principal_name}`\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "create_schemas",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

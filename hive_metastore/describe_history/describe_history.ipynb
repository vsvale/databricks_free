{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "989afe95-3e08-439e-bf25-73d17fce1786",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "par_database = getArgument(\"par_database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a114ef7-1297-4a01-9cf2-5ca722ba6947",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "from pyspark.sql.functions import collect_set,expr,col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64070040-a26c-4ca9-bb5d-935f6e310091",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"USE `{par_database}`\")\n",
    "table_list = spark.sql(\"SHOW TABLES\").filter(\"tableName != '_sqldf'\").selectExpr(f\"collect_set(concat('hive_metastore', '.', '{par_database}', '.', tableName)) as table_list\").first().table_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8be16dcc-f4ae-4c74-83a4-4e293ffd071a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a lock for thread-safe operations\n",
    "write_lock = threading.Lock()\n",
    "\n",
    "# List to accumulate rows\n",
    "accumulated_rows = []\n",
    "batch_size = 100\n",
    "\n",
    "# Set concurrency level\n",
    "concurrency_level = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38081c45-eaf1-4bf7-b3ec-25a1796c9cf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def describe_table(table_fqn):\n",
    "  catalog, database, tableName = table_fqn.split('.')\n",
    "  return spark.sql(f\"\"\"\n",
    "            WITH history AS (\n",
    "                DESCRIBE HISTORY {database}.{tableName} LIMIT 100\n",
    "            ),\n",
    "            job_exp AS (\n",
    "                SELECT operation, job.*, timestamp FROM history where operation not in (\"FSCK\",\"CONVERT\",\"OPTIMIZE\",\"CLONE\",\"RESTORE\",\"VACUUM\")\n",
    "            ),\n",
    "            dist_job AS (\n",
    "                SELECT DISTINCT operation, jobId, jobName, timestamp FROM job_exp\n",
    "            )\n",
    "            SELECT '{database}' AS database, '{tableName}' AS tableName, operation, jobId, jobName, max(timestamp)  FROM dist_job group by all\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9594db44-cd2a-4159-b4a7-a012ed933600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_table(rows):\n",
    "  schema = StructType([\n",
    "        StructField(\"database\", StringType(), True),\n",
    "        StructField(\"tableName\", StringType(), True),\n",
    "        StructField(\"operation\", StringType(), True),\n",
    "        StructField(\"jobId\", StringType(), True),\n",
    "        StructField(\"jobName\", StringType(), True),\n",
    "        StructField(\"timestamp\", StringType(), True),\n",
    "    ])\n",
    "  df = spark.createDataFrame(rows,schema=schema)\n",
    "  df.write.mode(\"append\").saveAsTable(\"dataops_prd.migration.tables_jobs_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e94de2c-1d7a-4385-bfa7-df21467547dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def describe_and_accumulate(table_fqn):\n",
    "    global accumulated_rows\n",
    "    result = describe_table(table_fqn)\n",
    "    for column in result.columns:\n",
    "        result = result.withColumn(column, col(column).cast(\"string\"))\n",
    "\n",
    "    rows = result.collect()\n",
    "\n",
    "    # Use lock to ensure thread-safe operations\n",
    "    with write_lock:\n",
    "        accumulated_rows.extend(rows)\n",
    "        if len(accumulated_rows) >= batch_size:\n",
    "            write_to_table(accumulated_rows)\n",
    "            accumulated_rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be253b00-6610-42b8-957c-953336e07fd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use ThreadPoolExecutor for concurrent execution\n",
    "with ThreadPoolExecutor(max_workers=concurrency_level) as executor:\n",
    "    # Submit tasks to the thread pool\n",
    "    future_to_table = {executor.submit(describe_and_accumulate, table): table for table in table_list}\n",
    "\n",
    "    # Wait for all tasks to complete\n",
    "    for future in as_completed(future_to_table):\n",
    "        item = future_to_table[future]\n",
    "        try:\n",
    "            future.result()  # Get result or exception\n",
    "        except Exception as exc:\n",
    "            print(f'{item} generated an exception')\n",
    "\n",
    "# Write any remaining rows\n",
    "if accumulated_rows:\n",
    "    write_to_table(accumulated_rows)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "describe_history",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
